{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import RobertaTokenizerFast as RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup, RobertaForSequenceClassification\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy, F1Score, AUROC\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu117\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>toxic_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww matches background colour im seemingly st...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man im really trying edit war guy constant...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir hero chance remember page thats</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  explanation edits made username hardcore metal...      0             0   \n",
       "1  daww matches background colour im seemingly st...      0             0   \n",
       "2  hey man im really trying edit war guy constant...      0             0   \n",
       "3  make real suggestions improvement wondered sec...      0             0   \n",
       "4                sir hero chance remember page thats      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  toxic_flag  \n",
       "0        0       0       0              0           0  \n",
       "1        0       0       0              0           0  \n",
       "2        0       0       0              0           0  \n",
       "3        0       0       0              0           0  \n",
       "4        0       0       0              0           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Archive\\\\clean_data.csv\").drop(columns=['Unnamed: 0', 'comment_length', 'id'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label columns:  ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "LABEL_COLUMNS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "num_labels = len(LABEL_COLUMNS)\n",
    "print('Label columns: ', LABEL_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>toxic_flag</th>\n",
       "      <th>one_hot_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>featured article think links incorporated feat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>read article</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>article interesting reasons trying promote con...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dyk nom know updated article lucy jane bledsoe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>want ill try refactor page removing gts remark...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  featured article think links incorporated feat...      0             0   \n",
       "1                                       read article      0             0   \n",
       "2  article interesting reasons trying promote con...      0             0   \n",
       "3  dyk nom know updated article lucy jane bledsoe...      0             0   \n",
       "4  want ill try refactor page removing gts remark...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  toxic_flag      one_hot_labels  \n",
       "0        0       0       0              0           0  [0, 0, 0, 0, 0, 0]  \n",
       "1        0       0       0              0           0  [0, 0, 0, 0, 0, 0]  \n",
       "2        0       0       0              0           0  [0, 0, 0, 0, 0, 0]  \n",
       "3        0       0       0              0           0  [0, 0, 0, 0, 0, 0]  \n",
       "4        0       0       0              0           0  [0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df['one_hot_labels'] = list(df[LABEL_COLUMNS].values)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142695, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df[df['toxic_flag'] == 1], df[df['toxic_flag'] == 0].sample((70000 - df[df['toxic_flag'] == 1]['toxic_flag'].sum()), random_state=42)]).drop(columns='toxic_flag').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 666,
       "width": 1067
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[LABEL_COLUMNS].sum().sort_values().plot(kind=\"barh\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      " strat_label\n",
      "0-0-0-0-0-0    56295\n",
      "1-0-0-0-0-0     4771\n",
      "1-0-1-0-1-0     3149\n",
      "1-0-1-0-0-0     1415\n",
      "1-0-0-0-1-0      999\n",
      "1-1-1-0-1-0      760\n",
      "1-0-1-0-1-1      479\n",
      "0-0-1-0-0-0      280\n",
      "0-0-0-0-1-0      273\n",
      "1-0-1-1-1-0      236\n",
      "1-1-1-0-1-1      197\n",
      "0-0-1-0-1-0      165\n",
      "1-0-0-1-0-0      134\n",
      "1-0-1-1-1-1      123\n",
      "1-1-1-0-0-0      105\n",
      "1-0-0-0-0-1       91\n",
      "1-1-1-1-1-0       91\n",
      "1-0-0-0-1-1       88\n",
      "1-1-1-1-1-1       58\n",
      "0-0-0-0-0-1       50\n",
      "0-0-0-1-0-0       43\n",
      "1-1-0-0-0-0       29\n",
      "1-0-1-0-0-1       26\n",
      "0-0-0-0-1-1       26\n",
      "1-0-0-1-1-0       17\n",
      "1-1-0-1-0-0       15\n",
      "0-0-1-0-1-1       15\n",
      "1-0-1-1-0-0       15\n",
      "1-0-0-1-0-1       12\n",
      "1-1-0-0-1-0        7\n",
      "0-0-1-1-1-0        6\n",
      "0-0-1-1-0-0        6\n",
      "1-0-0-1-1-1        5\n",
      "1-1-1-1-0-0        4\n",
      "1-1-0-1-1-0        3\n",
      "0-0-1-0-0-1        3\n",
      "1-1-0-1-0-1        3\n",
      "1-1-1-0-0-1        2\n",
      "0-0-0-1-1-0        2\n",
      "1-1-0-0-1-1        1\n",
      "1-1-0-0-0-1        1\n",
      "Name: count, dtype: int64\n",
      "Warning: At least one class has fewer than 2 instances.\n"
     ]
    }
   ],
   "source": [
    "df['strat_label'] = df['one_hot_labels'].apply(lambda x: '-'.join(map(str, x)))\n",
    "\n",
    "# Check distribution of stratification labels\n",
    "label_counts = df['strat_label'].value_counts()\n",
    "print(\"Class distribution:\\n\", label_counts)\n",
    "\n",
    "# If any class has only 1 instance, consider handling it:\n",
    "if (label_counts < 2).any():\n",
    "    print(\"Warning: At least one class has fewer than 2 instances.\")\n",
    "\n",
    "    # Option 1: Randomly split without stratification\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    strat_train_set, strat_test_set = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "else:\n",
    "    # Initialize StratifiedShuffleSplit if all classes are valid\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Apply stratified split\n",
    "    for train_index, test_index in sss.split(df, df['strat_label']):\n",
    "        strat_train_set = df.iloc[train_index]\n",
    "        strat_test_set = df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = strat_test_set\n",
    "train_df = strat_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN_COUNT = 128\n",
    "MODEL_NAME = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler_dataloader_creation(batch_size, df, tokenizer, label_columns, sampler='Sequential'):\n",
    "    \"\"\" Convert a DataFrame into a PyTorch DataLoader. \"\"\"\n",
    "\n",
    "    # Extract the relevant columns\n",
    "    inputs = df['comment_text'].tolist()\n",
    "    labels = df[label_columns].values.tolist()  # Convert to list\n",
    "    masks = df['one_hot_labels'].tolist()\n",
    "\n",
    "    # Tokenize the inputs\n",
    "    encoding = tokenizer(inputs, padding=True, truncation=True, return_tensors='pt')  # Use 'pt' for PyTorch tensors\n",
    "\n",
    "    # Convert labels and masks to tensors\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    masks = torch.tensor(np.array(masks))  # Convert to NumPy array first\n",
    "\n",
    "    # Create a TensorDataset and DataLoader\n",
    "    data = TensorDataset(\n",
    "        encoding['input_ids'],\n",
    "        encoding['attention_mask'],\n",
    "        masks,\n",
    "        labels\n",
    "    )\n",
    "    \n",
    "    if sampler == 'Sequential':\n",
    "        sampler = SequentialSampler(data)\n",
    "    elif sampler == 'Random':\n",
    "        sampler = RandomSampler(data)\n",
    "\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "# Example usage\n",
    "#train_dataloader = sampler_dataloader_creation(BATCH_SIZE, train_df, tokenizer, label_columns=LABEL_COLUMNS, sampler='Sequential')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataloader = sampler_dataloader_creation(BATCH_SIZE, train_df, tokenizer, sampler='Sequential')\n",
    "#validation_dataloader = sampler_dataloader_creation(BATCH_SIZE, val_df, tokenizer, label_columns=LABEL_COLUMNS, sampler='Sequential')\n",
    "#sampler_dataloader_creation(BATCH_SIZE, val_df, tokenizer, sampler='Random')\n",
    "\n",
    "\n",
    "# torch.save(validation_dataloader, 'validation_data_loader')\n",
    "# torch.save(train_dataloader, 'train_data_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
       "       'identity_hate', 'one_hot_labels', 'strat_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights calculation\n",
    "class_counts = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum().to_list()\n",
    "total_count = df.shape[0]\n",
    "class_weights = [total_count / count if count > 0 else 0 for count in class_counts]\n",
    "class_weights = np.array(class_weights) / np.sum(class_weights)\n",
    "class_weights_tensor = torch.FloatTensor(class_weights)\n",
    "\n",
    "class_weights_tensor = torch.FloatTensor(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHARTGPT\n",
    "\n",
    "Great! The shapes of the logits and labels now match, both being [16, 6]. This indicates that your model is correctly set up for multi-label classification.\n",
    "\n",
    "Next Steps\n",
    "* Loss Calculation: Since the shapes are correct, the model should be able to compute the loss without issues. If you still encounter errors, please check the following:\n",
    "\n",
    "* Ensure that there are no NaN or Inf values in either the logits or labels.\n",
    "* Make sure the loss function (BCEWithLogitsLoss) is correctly applied.\n",
    "* Training Output: As the training proceeds, monitor the loss values printed in the logs. If they appear reasonable, it means the training is on the right track.\n",
    "\n",
    "* Evaluate Training Performance: After training completes, you might want to evaluate your model's performance on a validation dataset to ensure it's learning effectively.\n",
    "\n",
    "Possible Improvements\n",
    "* Early Stopping: Consider implementing early stopping if you notice the training loss decreasing but the validation loss not improving.\n",
    "\n",
    "* Learning Rate Scheduler: Adding a learning rate scheduler can help improve convergence.\n",
    "\n",
    "* Model Checkpoints: Use checkpoints to save the best model based on validation performance.\n",
    "\n",
    "* Hyperparameter Tuning: Experiment with different batch sizes, learning rates, and the number of epochs to find the best configuration for your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df.sample(300, random_state=42)\n",
    "\n",
    "val_df = val_df.sample(100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torchmetrics import F1Score\n",
    "import numpy as np\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReBERTaClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights) if class_weights is not None else torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Initialize F1Score for multiclass task\n",
    "        self.f1_score_metric = F1Score(num_classes=num_classes, average='macro', task='multiclass').to(self.device)\n",
    "        self.val_outputs = []  # Initialize for storing validation outputs\n",
    "\n",
    "    def set_datasets(self, train_dataset, val_dataset):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        # Store the outputs for F1 calculation\n",
    "        preds = torch.sigmoid(logits)\n",
    "        preds = (preds > 0.5).float()\n",
    "        self.val_outputs.append((preds, labels))\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if not self.val_outputs:\n",
    "            self.log('avg_f1_score', 0.0)  # Log a default value if there are no outputs\n",
    "            return\n",
    "        \n",
    "        preds, labels = zip(*self.val_outputs)  # Unzip stored outputs\n",
    "        preds = torch.cat(preds)\n",
    "        labels = torch.cat(labels)\n",
    "\n",
    "        # Calculate and log F1 score\n",
    "        f1_score = self.f1_score_metric(preds, labels)  # Use the metric directly\n",
    "        self.log('avg_f1_score', f1_score)\n",
    "\n",
    "        # Reset for next epoch\n",
    "        self.val_outputs = []\n",
    "\n",
    "    def compute_f1(self, preds, labels):\n",
    "        # Implement F1 calculation here, e.g., using torchmetrics\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                             | Params | Mode \n",
      "-----------------------------------------------------------------------------\n",
      "0 | model           | RobertaForSequenceClassification | 124 M  | eval \n",
      "1 | criterion       | BCEWithLogitsLoss                | 0      | train\n",
      "2 | f1_score_metric | MulticlassF1Score                | 0      | train\n",
      "-----------------------------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.601   Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "230       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 19/19 [00:06<00:00,  3.09it/s, v_num=68, val_loss=0.320]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_f1_score improved. New best score: 0.480\n",
      "Epoch 0, global step 19: 'avg_f1_score' reached 0.47962 (best 0.47962), saving model to 'C:\\\\Users\\\\User\\\\Desktop\\\\Gintares_Projektai\\\\Toxic-Comment-Classification\\\\checkpoints\\\\best_model-epoch=00-avg_f1_score=0.48.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 19/19 [00:05<00:00,  3.17it/s, v_num=68, val_loss=0.255]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 38: 'avg_f1_score' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|| 19/19 [00:05<00:00,  3.18it/s, v_num=68, val_loss=0.233]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 57: 'avg_f1_score' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|| 19/19 [00:05<00:00,  3.17it/s, v_num=68, val_loss=0.193]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_f1_score improved by 0.143 >= min_delta = 0.0. New best score: 0.622\n",
      "Epoch 3, global step 76: 'avg_f1_score' reached 0.62242 (best 0.62242), saving model to 'C:\\\\Users\\\\User\\\\Desktop\\\\Gintares_Projektai\\\\Toxic-Comment-Classification\\\\checkpoints\\\\best_model-epoch=03-avg_f1_score=0.62.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 19/19 [00:06<00:00,  3.15it/s, v_num=68, val_loss=0.190]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_f1_score improved by 0.044 >= min_delta = 0.0. New best score: 0.667\n",
      "Epoch 4, global step 95: 'avg_f1_score' reached 0.66667 (best 0.66667), saving model to 'C:\\\\Users\\\\User\\\\Desktop\\\\Gintares_Projektai\\\\Toxic-Comment-Classification\\\\checkpoints\\\\best_model-epoch=04-avg_f1_score=0.67.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 19/19 [00:12<00:00,  1.58it/s, v_num=68, val_loss=0.190]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load your DataFrame `train_df` and `val_df`\n",
    "    texts_train = train_df['comment_text'].tolist()\n",
    "    labels_train = np.array(train_df['one_hot_labels'].tolist())\n",
    "\n",
    "    texts_val = val_df['comment_text'].tolist()\n",
    "    labels_val = np.array(val_df['one_hot_labels'].tolist())\n",
    "\n",
    "    num_labels = labels_train.shape[1]  # Ensure this is the correct number of classes\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", clean_up_tokenization_spaces=True)\n",
    "    max_length = 128  # Define MAX_TOKEN_COUNT as needed\n",
    "\n",
    "    # Create the training and validation datasets\n",
    "    train_dataset = TextDataset(texts_train, labels_train, tokenizer, max_length)\n",
    "    val_dataset = TextDataset(texts_val, labels_val, tokenizer, max_length)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ReBERTaClassifier(num_classes=num_labels)\n",
    "    model.set_datasets(train_dataset, val_dataset)  # Set the datasets\n",
    "\n",
    "    # Initialize the logger\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "    # Define EarlyStopping and ModelCheckpoint\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"avg_f1_score\",\n",
    "        mode=\"max\",\n",
    "        patience=3,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"avg_f1_score\",\n",
    "        mode=\"max\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"best_model-{epoch:02d}-{avg_f1_score:.2f}\"\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer with logger and callbacks\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=5, \n",
    "        logger=logger, \n",
    "        callbacks=[early_stopping, checkpoint_callback],\n",
    "        log_every_n_steps=1,\n",
    "        # gpus=1 if torch.cuda.is_available() else 0  # Uncomment to use GPU if available\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir=logs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ReBERTaClassifier.__init__() got an unexpected keyword argument 'train_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m TextDataset(texts_val, labels_val, tokenizer, max_length)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Initialize the model with datasets\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mReBERTaClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Initialize the logger\u001b[39;00m\n\u001b[0;32m     23\u001b[0m logger \u001b[38;5;241m=\u001b[39m TensorBoardLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtb_logs\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: ReBERTaClassifier.__init__() got an unexpected keyword argument 'train_dataset'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load your DataFrame `train_df` and `val_df`\n",
    "    texts_train = train_df['comment_text'].tolist()\n",
    "    labels_train = np.array(train_df['one_hot_labels'].tolist())\n",
    "\n",
    "    texts_val = val_df['comment_text'].tolist()\n",
    "    labels_val = np.array(val_df['one_hot_labels'].tolist())\n",
    "\n",
    "    num_labels = labels_train.shape[1]  # Ensure this is the correct number of classes\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", clean_up_tokenization_spaces=True)\n",
    "    max_length = 128  # Define MAX_TOKEN_COUNT as needed\n",
    "\n",
    "    # Create the training and validation datasets\n",
    "    train_dataset = TextDataset(texts_train, labels_train, tokenizer, max_length)\n",
    "    val_dataset = TextDataset(texts_val, labels_val, tokenizer, max_length)\n",
    "\n",
    "    # Initialize the model with datasets\n",
    "    model = ReBERTaClassifier(num_classes=num_labels, train_dataset=train_dataset, val_dataset=val_dataset)\n",
    "\n",
    "    # Initialize the logger\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "    # Define EarlyStopping and ModelCheckpoint\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"avg_f1_score\",\n",
    "        mode=\"max\",\n",
    "        patience=3,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"avg_f1_score\",\n",
    "        mode=\"max\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"best_model-{epoch:02d}-{avg_f1_score:.2f}\"\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer with logger and callbacks\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=5, \n",
    "        logger=logger, \n",
    "        callbacks=[early_stopping, checkpoint_callback],\n",
    "        log_every_n_steps=1,\n",
    "        #gpus=1 if torch.cuda.is_available() else 0  # Uncomment to use GPU if available\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                             | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | model     | RobertaForSequenceClassification | 124 M  | eval \n",
      "1 | criterion | BCEWithLogitsLoss                | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.601   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "230       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|| 2/2 [00:00<00:00, 52.62it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'val_dataloader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 57\u001b[0m\n\u001b[0;32m     48\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m     49\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[0;32m     50\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# gpus=1 if torch.cuda.is_available() else 0  # Uncomment to use GPU if available\u001b[39;00m\n\u001b[0;32m     54\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Train the model with both training and validation DataLoaders\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1025\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1052\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1049\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1052\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:142\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_run_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:254\u001b[0m, in \u001b[0;36m_EvaluationLoop.on_run_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39m_evaluation_epoch_end()\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_evaluation_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m logged_outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logged_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logged_outputs, []  \u001b[38;5;66;03m# free memory\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# include any logged outputs on epoch_end\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:334\u001b[0m, in \u001b[0;36m_EvaluationLoop._on_evaluation_epoch_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    332\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    333\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, hook_name)\n\u001b[1;32m--> 334\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_epoch_end()\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    170\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "Cell \u001b[1;32mIn[64], line 107\u001b[0m, in \u001b[0;36mReBERTaClassifier.on_validation_epoch_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_validation_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# Combine outputs from all validation steps\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     all_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_outputs)\n\u001b[1;32m--> 107\u001b[0m     all_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_dataloader\u001b[49m()])  \u001b[38;5;66;03m# Adjust to your dataloader\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# Calculate F1 scores for each class and log\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf1_scores)):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'val_dataloader'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load your DataFrame `train_df` and `val_df`\n",
    "    texts_train = train_df['comment_text'].tolist()\n",
    "    labels_train = np.array(train_df['one_hot_labels'].tolist())\n",
    "\n",
    "    texts_val = val_df['comment_text'].tolist()\n",
    "    labels_val = np.array(val_df['one_hot_labels'].tolist())\n",
    "\n",
    "    num_labels = labels_train.shape[1]  # Ensure this is the correct number of classes\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", clean_up_tokenization_spaces=True)\n",
    "    max_length = 128  # Define MAX_TOKEN_COUNT as needed\n",
    "\n",
    "    # Create the training dataset and dataloader\n",
    "    train_dataset = TextDataset(texts_train, labels_train, tokenizer, max_length)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Create the validation dataset and dataloader\n",
    "    val_dataset = TextDataset(texts_val, labels_val, tokenizer, max_length)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ReBERTaClassifier(num_classes=num_labels)\n",
    "\n",
    "    # Initialize the logger\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "    # Define EarlyStopping and ModelCheckpoint\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"avg_f1_score\",   # Now monitoring F1 score from validation\n",
    "        mode=\"max\",               # We're maximizing F1 score\n",
    "        patience=3,               # Number of epochs with no improvement\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"avg_f1_score\",\n",
    "        mode=\"max\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"best_model-{epoch:02d}-{avg_f1_score:.2f}\"\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer with logger and callbacks\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=5, \n",
    "        logger=logger, \n",
    "        callbacks=[early_stopping, checkpoint_callback],\n",
    "        log_every_n_steps=1,\n",
    "        # gpus=1 if torch.cuda.is_available() else 0  # Uncomment to use GPU if available\n",
    "    )\n",
    "\n",
    "    # Train the model with both training and validation DataLoaders\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                             | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | model     | RobertaForSequenceClassification | 124 M  | eval \n",
      "1 | criterion | BCEWithLogitsLoss                | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.601   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "230       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 19/19 [00:05<00:00,  3.22it/s, v_num=55, class_0_loss_step=0.206, class_1_loss_step=0.0378, class_2_loss_step=0.0734, class_3_loss_step=0.044, class_4_loss_step=0.0817, class_5_loss_step=0.0459] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 19/19 [00:11<00:00,  1.63it/s, v_num=55, class_0_loss_step=0.206, class_1_loss_step=0.0378, class_2_loss_step=0.0734, class_3_loss_step=0.044, class_4_loss_step=0.0817, class_5_loss_step=0.0459]\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torchmetrics import F1Score\n",
    "import numpy as np\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])\n",
    "    }\n",
    "\n",
    "\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "\n",
    "class ReBERTaClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes, class_weights=None):\n",
    "        super(ReBERTaClassifier, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights) if class_weights is not None else torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Initialize F1 score for each class\n",
    "        self.f1_scores = {i: BinaryF1Score().to(self.device) for i in range(num_classes)}\n",
    "        self.class_losses = {i: [] for i in range(num_classes)}\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        # Move F1 scores to the correct device\n",
    "        for i in range(len(self.f1_scores)):\n",
    "            self.f1_scores[i] = self.f1_scores[i].to(self.device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = self(input_ids, attention_mask)\n",
    "\n",
    "        # Calculate overall loss\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        # Compute per-class loss and update F1 scores\n",
    "        preds = torch.sigmoid(logits)\n",
    "        preds = (preds > 0.5).float()\n",
    "\n",
    "        for i in range(labels.shape[1]):\n",
    "            class_logits = logits[:, i]\n",
    "            class_labels = labels[:, i]\n",
    "            class_loss = self.criterion(class_logits, class_labels)\n",
    "            self.log(f'class_{i}_loss_step', class_loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "\n",
    "            # Update F1 score for each class\n",
    "            self.f1_scores[i].update(preds[:, i], class_labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Log F1 scores for each class and compute average\n",
    "        avg_f1_score = 0\n",
    "        for i in range(len(self.f1_scores)):\n",
    "            f1_score = self.f1_scores[i].compute()\n",
    "            self.log(f'class_{i}_f1', f1_score, on_epoch=True, prog_bar=True)\n",
    "            avg_f1_score += f1_score\n",
    "            self.f1_scores[i].reset()  # Reset F1 score for the next epoch\n",
    "\n",
    "        # Log the average F1 score across all classes\n",
    "        avg_f1_score /= len(self.f1_scores)\n",
    "        self.log('avg_f1_score', avg_f1_score, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "\n",
    "# Sample usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your DataFrame `train_df`\n",
    "    texts = train_df['comment_text'].tolist()\n",
    "    labels = np.array(train_df['one_hot_labels'].tolist())\n",
    "\n",
    "    num_labels = labels.shape[1]  # Ensure this is the correct number of classes\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", clean_up_tokenization_spaces=True)\n",
    "    max_length = 128  # Define MAX_TOKEN_COUNT as needed\n",
    "\n",
    "    # Create the dataset and dataloader\n",
    "    dataset = TextDataset(texts, labels, tokenizer, max_length)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ReBERTaClassifier(num_classes=num_labels)\n",
    "\n",
    "    # Initialize the logger\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "    # Initialize the trainer with the logger\n",
    "    trainer = pl.Trainer(max_epochs=5, logger=logger)\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                             | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | model     | RobertaForSequenceClassification | 124 M  | eval \n",
      "1 | criterion | BCEWithLogitsLoss                | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.601   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "230       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 19/19 [00:06<00:00,  3.15it/s, v_num=52, class_0_loss_step=0.314, class_1_loss_step=0.0519, class_2_loss_step=0.064, class_3_loss_step=0.048, class_4_loss_step=0.0686, class_5_loss_step=0.0536, class_0_loss_epoch=0.239, class_1_loss_epoch=0.0787, class_2_loss_epoch=0.169, class_3_loss_epoch=0.0709, class_4_loss_epoch=0.180, class_5_loss_epoch=0.0762]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 19/19 [00:11<00:00,  1.60it/s, v_num=52, class_0_loss_step=0.314, class_1_loss_step=0.0519, class_2_loss_step=0.064, class_3_loss_step=0.048, class_4_loss_step=0.0686, class_5_loss_step=0.0536, class_0_loss_epoch=0.239, class_1_loss_epoch=0.0787, class_2_loss_epoch=0.169, class_3_loss_epoch=0.0709, class_4_loss_epoch=0.180, class_5_loss_epoch=0.0762]\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torchmetrics import F1Score\n",
    "import numpy as np\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])\n",
    "    }\n",
    "\n",
    "class ReBERTaClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ReBERTaClassifier, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        self.class_losses = {i: [] for i in range(num_classes)}\n",
    "\n",
    "        # Initialize F1 metrics for each class\n",
    "        self.f1_scores = {i: F1Score(num_classes=1, threshold=0.5, task='binary') for i in range(num_classes)}\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        # Move F1 scores to the correct device\n",
    "        for i in range(len(self.f1_scores)):\n",
    "            self.f1_scores[i] = self.f1_scores[i].to(self.device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "    # def training_step(self, batch, batch_idx):\n",
    "    #     input_ids = batch['input_ids']\n",
    "    #     attention_mask = batch['attention_mask']\n",
    "    #     labels = batch['labels']\n",
    "\n",
    "    #     logits = self(input_ids, attention_mask)\n",
    "\n",
    "    #     # Calculate overall loss\n",
    "    #     loss = self.criterion(logits, labels)\n",
    "\n",
    "    #     # Compute and log per-class loss\n",
    "    #     for i in range(labels.shape[1]):\n",
    "    #         class_logits = logits[:, i]\n",
    "    #         class_labels = labels[:, i]\n",
    "    #         class_loss = self.criterion(class_logits, class_labels)\n",
    "            \n",
    "    #         # Log per-class loss using a distinct tag for each class\n",
    "    #         self.log(f'class_{i}_loss', class_loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    #     return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = self(input_ids, attention_mask)\n",
    "\n",
    "        # Calculate overall loss\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        # Compute and log per-class loss\n",
    "        preds = torch.sigmoid(logits)\n",
    "        preds = (preds > 0.5).float()  # Convert logits to binary predictions using threshold 0.5\n",
    "\n",
    "        for i in range(labels.shape[1]):\n",
    "            class_logits = logits[:, i]\n",
    "            class_labels = labels[:, i]\n",
    "            class_loss = self.criterion(class_logits, class_labels)\n",
    "            \n",
    "            # Log per-class loss using a distinct tag for each class\n",
    "            self.log(f'class_{i}_loss', class_loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "            # Update F1 score metric for each class\n",
    "            self.f1_scores[i](preds[:, i], class_labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # def on_epoch_end(self):\n",
    "    #     # Log F1 scores for each class\n",
    "    #     f1_scores = []\n",
    "    #     for i in range(len(self.f1_scores)):\n",
    "    #         f1_score = self.f1_scores[i].compute()\n",
    "    #         f1_scores.append(f1_score)\n",
    "    #         self.f1_scores[i].reset()  # Reset F1 score for next epoch\n",
    "\n",
    "    #     # Log the average F1 score across all classes\n",
    "    #     avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "    #     self.log('avg_f1_score', avg_f1_score)\n",
    "\n",
    "    #     for class_idx, losses in self.class_losses.items():\n",
    "    #         avg_loss = sum(losses) / len(losses) if losses else 0\n",
    "    #         self.log(f'class_{class_idx}_loss_avg', avg_loss)\n",
    "    #     self.class_losses = {i: [] for i in range(len(self.class_losses))}\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Log F1 scores for each class\n",
    "        for i in range(len(self.f1_scores)):\n",
    "            f1_score = self.f1_scores[i].compute()\n",
    "            self.log(f'class_{i}_f1', f1_score, on_epoch=True, prog_bar=True)\n",
    "            self.f1_scores[i].reset()  # Reset F1 score for next epoch\n",
    "\n",
    "        # Also log the average F1 score across all classes\n",
    "        avg_f1_score = sum([self.f1_scores[i].compute() for i in range(len(self.f1_scores))]) / len(self.f1_scores)\n",
    "        self.log('avg_f1_score', avg_f1_score)\n",
    "\n",
    "        # Reset the loss storage for the next epoch\n",
    "        for class_idx, losses in self.class_losses.items():\n",
    "            avg_loss = sum(losses) / len(losses) if losses else 0\n",
    "            self.log(f'class_{class_idx}_loss_avg', avg_loss)\n",
    "        self.class_losses = {i: [] for i in range(len(self.class_losses))}\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "# Sample usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your DataFrame `train_df`\n",
    "    texts = train_df['comment_text'].tolist()\n",
    "    labels = np.array(train_df['one_hot_labels'].tolist())\n",
    "\n",
    "    num_labels = labels.shape[1]  # Ensure this is the correct number of classes\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", clean_up_tokenization_spaces=True)\n",
    "    max_length = 128  # Define MAX_TOKEN_COUNT as needed\n",
    "\n",
    "    # Create the dataset and dataloader\n",
    "    dataset = TextDataset(texts, labels, tokenizer, max_length)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ReBERTaClassifier(num_classes=num_labels)\n",
    "\n",
    "    # Initialize the logger\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "    # Initialize the trainer with the logger\n",
    "    trainer = pl.Trainer(max_epochs=5, logger=logger)\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                             | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | model     | RobertaForSequenceClassification | 124 M  | eval \n",
      "1 | criterion | BCEWithLogitsLoss                | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.601   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "230       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/3500 [21:11<?, ?it/s]17it/s, v_num=50]\n",
      "Epoch 0:   0%|          | 0/19 [13:41<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/19 [12:53<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/19 [12:27<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/19 [12:17<?, ?it/s]\n",
      "Epoch 4: 100%|| 19/19 [01:02<00:00,  0.31it/s, v_num=50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 19/19 [01:10<00:00,  0.27it/s, v_num=50]\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torchmetrics import F1Score\n",
    "import numpy as np\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])\n",
    "    }\n",
    "\n",
    "class ReBERTaClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ReBERTaClassifier, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        self.class_losses = {i: [] for i in range(num_classes)}\n",
    "\n",
    "        # Initialize F1 metrics for each class\n",
    "        self.f1_scores = {i: F1Score(num_classes=1, threshold=0.5, task='binary') for i in range(num_classes)}\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        # Move F1 scores to the correct device\n",
    "        for i in range(len(self.f1_scores)):\n",
    "            self.f1_scores[i] = self.f1_scores[i].to(self.device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = self(input_ids, attention_mask)\n",
    "\n",
    "        # Calculate overall loss\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        # Compute per-class loss and log\n",
    "        for i in range(labels.shape[1]):\n",
    "            class_logits = logits[:, i]\n",
    "            class_labels = labels[:, i]\n",
    "            class_loss = self.criterion(class_logits, class_labels)\n",
    "            self.class_losses[i].append(class_loss.item())\n",
    "            self.log(f'class_{i}_loss', class_loss.detach())\n",
    "\n",
    "        # Calculate F1 score for each class\n",
    "        preds = torch.sigmoid(logits)\n",
    "        preds = (preds > 0.5).float()  # Thresholding\n",
    "        \n",
    "        for i in range(labels.shape[1]):\n",
    "            self.f1_scores[i](preds[:, i], labels[:, i])  # Update F1 score\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # def on_epoch_end(self):\n",
    "    #     # Log F1 scores for each class\n",
    "    #     for i in range(len(self.f1_scores)):\n",
    "    #         f1_score = self.f1_scores[i].compute()\n",
    "    #         self.log(f'class_{i}_f1', f1_score)\n",
    "    #         self.f1_scores[i].reset()  # Reset F1 score for next epoch\n",
    "\n",
    "    #     for class_idx, losses in self.class_losses.items():\n",
    "    #         avg_loss = sum(losses) / len(losses) if losses else 0\n",
    "    #         self.log(f'class_{class_idx}_loss_avg', avg_loss)\n",
    "    #     self.class_losses = {i: [] for i in range(len(self.class_losses))}\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Log F1 scores for each class\n",
    "        f1_scores = []\n",
    "        for i in range(len(self.f1_scores)):\n",
    "            f1_score = self.f1_scores[i].compute()\n",
    "            f1_scores.append(f1_score)\n",
    "            self.f1_scores[i].reset()  # Reset F1 score for next epoch\n",
    "\n",
    "        # Log the average F1 score across all classes\n",
    "        avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "        self.log('avg_f1_score', avg_f1_score)\n",
    "\n",
    "        for class_idx, losses in self.class_losses.items():\n",
    "            avg_loss = sum(losses) / len(losses) if losses else 0\n",
    "            self.log(f'class_{class_idx}_loss_avg', avg_loss)\n",
    "        self.class_losses = {i: [] for i in range(len(self.class_losses))}\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "# Sample usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your DataFrame `train_df`\n",
    "    texts = train_df['comment_text'].tolist()\n",
    "    labels = np.array(train_df['one_hot_labels'].tolist())\n",
    "\n",
    "    num_labels = labels.shape[1]  # Ensure this is the correct number of classes\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", clean_up_tokenization_spaces=True)\n",
    "    max_length = 128  # Define MAX_TOKEN_COUNT as needed\n",
    "\n",
    "    # Create the dataset and dataloader\n",
    "    dataset = TextDataset(texts, labels, tokenizer, max_length)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ReBERTaClassifier(num_classes=num_labels)\n",
    "\n",
    "    # Initialize the logger\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "    # Initialize the trainer with the logger\n",
    "    trainer = pl.Trainer(max_epochs=5, logger=logger)\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                             | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | model     | RobertaForSequenceClassification | 124 M  | eval \n",
      "1 | criterion | BCEWithLogitsLoss                | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.601   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "230       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 19/19 [00:05<00:00,  3.35it/s, v_num=44]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 19/19 [00:11<00:00,  1.60it/s, v_num=44]\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])\n",
    "    }\n",
    "\n",
    "class ReBERTaClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ReBERTaClassifier, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        self.class_losses = {i: [] for i in range(num_classes)}\n",
    "        self.all_labels = []\n",
    "        self.all_preds = []\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = self(input_ids, attention_mask)\n",
    "\n",
    "        # Calculate overall loss\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        # Store predictions and labels for F1 calculation\n",
    "        self.all_labels.append(labels.cpu())\n",
    "        self.all_preds.append(torch.sigmoid(logits).cpu())\n",
    "\n",
    "        # Compute per-class loss and log\n",
    "        for i in range(labels.shape[1]):\n",
    "            class_logits = logits[:, i]\n",
    "            class_labels = labels[:, i]\n",
    "            class_loss = self.criterion(class_logits, class_labels)\n",
    "            self.class_losses[i].append(class_loss.item())\n",
    "            self.log(f'class_{i}_loss', class_loss.detach())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Calculate F1 score for each class\n",
    "        self.all_labels = torch.cat(self.all_labels)\n",
    "        self.all_preds = torch.cat(self.all_preds)\n",
    "        preds = (self.all_preds > 0.5).float()  # Thresholding\n",
    "\n",
    "        for i in range(self.all_labels.shape[1]):\n",
    "            f1 = f1_score(self.all_labels[:, i].numpy(), preds[:, i].numpy(), average='binary')\n",
    "            self.log(f'class_{i}_f1', f1)\n",
    "\n",
    "        for class_idx, losses in self.class_losses.items():\n",
    "            avg_loss = sum(losses) / len(losses) if losses else 0\n",
    "            self.log(f'class_{class_idx}_loss_avg', avg_loss)\n",
    "        self.class_losses = {i: [] for i in range(len(self.class_losses))}\n",
    "        \n",
    "        # Reset for next epoch\n",
    "        self.all_labels = []\n",
    "        self.all_preds = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "# Sample usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your DataFrame `train_df`\n",
    "    texts = train_df['comment_text'].tolist()\n",
    "    labels = np.array(train_df['one_hot_labels'].tolist())\n",
    "\n",
    "    num_labels = labels.shape[1]  # Ensure this is the correct number of classes\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", clean_up_tokenization_spaces=True)\n",
    "    max_length = 128  # Define MAX_TOKEN_COUNT as needed\n",
    "\n",
    "    # Create the dataset and dataloader\n",
    "    dataset = TextDataset(texts, labels, tokenizer, max_length)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ReBERTaClassifier(num_classes=num_labels)\n",
    "\n",
    "    # Initialize the logger\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "    # Initialize the trainer with the logger\n",
    "    trainer = pl.Trainer(max_epochs=5, logger=logger)\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/3500 [03:02<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/3500 [02:25<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/3500 [02:20<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/3500 [01:47<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                             | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | model     | RobertaForSequenceClassification | 124 M  | eval \n",
      "1 | criterion | BCEWithLogitsLoss                | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.601   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "230       Modules in eval mode\n",
      "c:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 19/19 [00:05<00:00,  3.37it/s, v_num=43]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 19/19 [00:11<00:00,  1.66it/s, v_num=43]\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import numpy as np\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])\n",
    "    }\n",
    "\n",
    "class ReBERTaClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ReBERTaClassifier, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        self.class_losses = {i: [] for i in range(num_classes)}\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = self(input_ids, attention_mask)\n",
    "\n",
    "        # Calculate overall loss\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        # Compute per-class loss and log\n",
    "        for i in range(labels.shape[1]):  # Assuming labels are one-hot encoded\n",
    "            class_logits = logits[:, i]  # Shape: [batch_size]\n",
    "            class_labels = labels[:, i]   # Shape: [batch_size]\n",
    "            class_loss = self.criterion(class_logits, class_labels)\n",
    "            self.class_losses[i].append(class_loss.item())\n",
    "            self.log(f'class_{i}_loss', class_loss.detach())  # Log per-class loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        for class_idx, losses in self.class_losses.items():\n",
    "            avg_loss = sum(losses) / len(losses) if losses else 0\n",
    "            self.log(f'class_{class_idx}_loss_avg', avg_loss)\n",
    "        self.class_losses = {i: [] for i in range(len(self.class_losses))}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "# Sample usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your DataFrame `train_df`\n",
    "    texts = train_df['comment_text'].tolist()\n",
    "    labels = np.array(train_df['one_hot_labels'].tolist())\n",
    "\n",
    "    num_labels = labels.shape[1]  # Ensure this is the correct number of classes\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", clean_up_tokenization_spaces=True)\n",
    "    max_length = 128  # Ensure MAX_TOKEN_COUNT is defined\n",
    "\n",
    "    # Create the dataset and dataloader\n",
    "    dataset = TextDataset(texts, labels, tokenizer, max_length)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ReBERTaClassifier(num_classes=num_labels)\n",
    "\n",
    "    # Initialize the logger\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "    # Initialize the trainer with the logger\n",
    "    trainer = pl.Trainer(max_epochs=5, logger=logger)\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                             | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | model     | RobertaForSequenceClassification | 124 M  | eval \n",
      "1 | criterion | BCEWithLogitsLoss                | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.601   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "230       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/3500 [00:00<?, ?it/s] Input IDs shape: torch.Size([16, 128])\n",
      "Attention Mask shape: torch.Size([16, 128])\n",
      "Labels shape: torch.Size([16, 6])\n",
      "Logits shape: torch.Size([16, 6]), Labels shape: torch.Size([16, 6])\n",
      "Overall Loss value: 0.6843869090080261\n",
      "Class 0 Logits shape: torch.Size([16]), Labels shape: torch.Size([16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (16) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 157\u001b[0m\n\u001b[0;32m    154\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, logger\u001b[38;5;241m=\u001b[39mlogger, log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1025\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         closure()\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[0;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    170\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1277\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1281\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1304\u001b[0m \n\u001b[0;32m   1305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1306\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\torch\\optim\\adamw.py:148\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 148\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    151\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     97\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     98\u001b[0m     optimizer: Steppable,\n\u001b[0;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m \n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m--> 317\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 98\u001b[0m, in \u001b[0;36mReBERTaClassifier.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Logits shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_logits\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Labels shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_labels\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Calculate the per-class loss\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m class_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_losses[i]\u001b[38;5;241m.\u001b[39mappend(class_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, class_loss\u001b[38;5;241m.\u001b[39mitem())  \u001b[38;5;66;03m# Log per-class loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:720\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\torch\\nn\\functional.py:3165\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m   3163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m-> 3165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (16) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])\n",
    "    }\n",
    "\n",
    "def calculate_class_weights(labels):\n",
    "    class_counts = labels.sum(axis=0)\n",
    "    total_samples = labels.shape[0]\n",
    "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "    class_weights[np.isinf(class_weights)] = 0\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "class ReBERTaClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes, class_weights=None):\n",
    "        super(ReBERTaClassifier, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights) if class_weights is not None else torch.nn.BCEWithLogitsLoss()\n",
    "        self.class_losses = {i: [] for i in range(num_classes)}\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels'].float()  # Ensure labels are float\n",
    "\n",
    "        # Debugging: Print shapes and check for NaN/Inf\n",
    "        print(f'Input IDs shape: {input_ids.shape}')\n",
    "        print(f'Attention Mask shape: {attention_mask.shape}')\n",
    "        print(f'Labels shape: {labels.shape}')\n",
    "\n",
    "        logits = self(input_ids, attention_mask)\n",
    "\n",
    "        # Check for NaN/Inf values\n",
    "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "            print(\"Logits contain NaN or Inf values.\")\n",
    "        if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "            print(\"Labels contain NaN or Inf values.\")\n",
    "\n",
    "        print(f'Logits shape: {logits.shape}, Labels shape: {labels.shape}')\n",
    "        \n",
    "        # Calculate overall loss\n",
    "        loss = self.criterion(logits, labels)\n",
    "        print(f'Overall Loss value: {loss.item()}')\n",
    "\n",
    "        # Compute per-class loss and log\n",
    "        with torch.no_grad():\n",
    "            for i in range(labels.shape[1]):  # Assuming labels are one-hot encoded\n",
    "                # Extract logits and labels for the current class\n",
    "                class_logits = logits[:, i]  # Shape: [16]\n",
    "                class_labels = labels[:, i]   # Shape: [16]\n",
    "                \n",
    "                # Check shapes before calculating loss\n",
    "                print(f'Class {i} Logits shape: {class_logits.shape}, Labels shape: {class_labels.shape}')\n",
    "\n",
    "                # Calculate the per-class loss\n",
    "                class_loss = self.criterion(class_logits, class_labels)\n",
    "                self.class_losses[i].append(class_loss.item())\n",
    "                self.log(f'class_{i}_loss', class_loss.item())  # Log per-class loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        for class_idx, losses in self.class_losses.items():\n",
    "            avg_loss = sum(losses) / len(losses) if losses else 0\n",
    "            self.log(f'class_{class_idx}_loss', avg_loss)\n",
    "        self.class_losses = {i: [] for i in range(len(self.class_losses))}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        for class_idx, losses in self.class_losses.items():\n",
    "            avg_loss = sum(losses) / len(losses) if losses else 0\n",
    "            self.log(f'class_{class_idx}_loss', avg_loss)\n",
    "        self.class_losses = {i: [] for i in range(len(self.class_losses))}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "# Sample usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your DataFrame `train_df`\n",
    "    #\n",
    "    # train_df = train_df.sample(100, random_state=42)\n",
    "    texts = train_df['comment_text'].tolist()\n",
    "    labels = np.array(train_df['one_hot_labels'].tolist())\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_weights = calculate_class_weights(labels)\n",
    "\n",
    "    # Define number of classes based on labels\n",
    "    num_labels = labels.shape[1]  # Ensure this is the correct number of classes\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", clean_up_tokenization_spaces=True)\n",
    "    max_length = MAX_TOKEN_COUNT  # Ensure MAX_TOKEN_COUNT is defined\n",
    "\n",
    "    # Create the dataset and dataloader\n",
    "    dataset = TextDataset(texts, labels, tokenizer, max_length)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ReBERTaClassifier(num_classes=num_labels, class_weights=class_weights)\n",
    "\n",
    "    # Initialize the logger\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "    # Initialize the trainer with the logger\n",
    "    trainer = pl.Trainer(max_epochs=5, logger=logger, log_every_n_steps=1)\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19260\\608962001.py:48: RuntimeWarning: divide by zero encountered in divide\n",
      "  class_weights = total_samples / (len(class_counts) * class_counts)\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                             | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | model     | RobertaForSequenceClassification | 124 M  | eval \n",
      "1 | criterion | BCEWithLogitsLoss                | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.601   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "230       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/7 [00:00<?, ?it/s] Logits shape: torch.Size([16, 6]), Labels shape: torch.Size([16, 6])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [16, 1] doesn't match the broadcast shape [16, 6]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 170\u001b[0m\n\u001b[0;32m    166\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, logger\u001b[38;5;241m=\u001b[39mlogger, log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1025\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         closure()\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[0;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    170\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1277\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1281\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1304\u001b[0m \n\u001b[0;32m   1305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1306\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\torch\\optim\\adamw.py:148\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 148\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    151\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     97\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     98\u001b[0m     optimizer: Steppable,\n\u001b[0;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m \n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m--> 317\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[56], line 117\u001b[0m, in \u001b[0;36mReBERTaClassifier.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):  \u001b[38;5;66;03m# Assuming labels are one-hot encoded\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m         class_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_losses[i]\u001b[38;5;241m.\u001b[39mappend(class_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:720\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\torch\\nn\\functional.py:3165\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m   3163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m-> 3165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: output with shape [16, 1] doesn't match the broadcast shape [16, 6]"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])\n",
    "    }\n",
    "\n",
    "def calculate_class_weights(labels):\n",
    "    class_counts = labels.sum(axis=0)\n",
    "    total_samples = labels.shape[0]\n",
    "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "    class_weights[np.isinf(class_weights)] = 0\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# class ReBERTaClassifier(pl.LightningModule):\n",
    "#     def __init__(self, num_classes, class_weights=None):\n",
    "#         super(ReBERTaClassifier, self).__init__()\n",
    "#         self.model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "#         self.criterion = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights) if class_weights is not None else torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "#         return outputs.logits\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         input_ids = batch['input_ids']\n",
    "#         attention_mask = batch['attention_mask']\n",
    "#         labels = batch['labels']\n",
    "\n",
    "#         logits = self(input_ids, attention_mask)\n",
    "#         loss = self.criterion(logits, labels)\n",
    "#         self.log('train_loss', loss)\n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "#         return optimizer\n",
    "\n",
    "\n",
    "class ReBERTaClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes, class_weights=None):\n",
    "        super(ReBERTaClassifier, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights) if class_weights is not None else torch.nn.BCEWithLogitsLoss()\n",
    "        self.class_losses = {i: [] for i in range(num_classes)}  # Store per-class losses\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "\n",
    "        # Convert labels to a PyTorch tensor if they are a NumPy array\n",
    "        labels = batch['labels']\n",
    "        if isinstance(labels, np.ndarray):\n",
    "            labels = torch.tensor(labels)  # Convert NumPy array to PyTorch tensor\n",
    "        \n",
    "        labels = labels.float()  # Ensure labels are of type float\n",
    "\n",
    "        logits = self(input_ids, attention_mask)\n",
    "\n",
    "        # Debugging: Print shapes\n",
    "        print(f'Logits shape: {logits.shape}, Labels shape: {labels.shape}')\n",
    "\n",
    "        # Check for NaN or Inf values\n",
    "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "            raise ValueError(\"Logits contain NaN or Inf values.\")\n",
    "\n",
    "        # Ensure dimensions are correct for BCEWithLogitsLoss\n",
    "        if logits.shape != labels.shape:\n",
    "            raise ValueError(f\"Shape mismatch: logits {logits.shape} vs labels {labels.shape}\")\n",
    "\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        # Compute per-class loss\n",
    "        with torch.no_grad():\n",
    "            for i in range(labels.shape[1]):  # Assuming labels are one-hot encoded\n",
    "                class_loss = self.criterion(logits[:, i].unsqueeze(1), labels[:, i].unsqueeze(1))\n",
    "                self.class_losses[i].append(class_loss.item())\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Log average per-class losses\n",
    "        for class_idx, losses in self.class_losses.items():\n",
    "            avg_loss = sum(losses) / len(losses) if losses else 0\n",
    "            self.log(f'class_{class_idx}_loss', avg_loss)\n",
    "\n",
    "        # Clear the losses for the next epoch\n",
    "        self.class_losses = {i: [] for i in range(len(self.class_losses))}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "# Sample usage\n",
    "#if __name__ == \"__main__\":\n",
    "# Load your DataFrame `train_df`\n",
    "train_df = train_df.sample(100, random_state=42)\n",
    "texts = train_df['comment_text'].tolist()\n",
    "labels = np.array(train_df['one_hot_labels'].tolist())\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = calculate_class_weights(labels)\n",
    "\n",
    "# Define number of classes based on labels\n",
    "num_labels = labels.shape[1]\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", clean_up_tokenization_spaces=True)\n",
    "max_length = MAX_TOKEN_COUNT\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = TextDataset(texts, labels, tokenizer, max_length)\n",
    "train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn, \n",
    "                                num_workers=0 # 1 GPU, paralelization creates a bottlenect\n",
    "                                )\n",
    "\n",
    "# Initialize the model\n",
    "model = ReBERTaClassifier(num_classes=num_labels, class_weights=class_weights)\n",
    "\n",
    "# Initialize the logger\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "# Initialize the trainer with the logger\n",
    "trainer = pl.Trainer(max_epochs=5, logger=logger, log_every_n_steps=1)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20756\\1032146655.py:52: RuntimeWarning: divide by zero encountered in divide\n",
      "  class_weights = total_samples / (len(class_counts) * class_counts)\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                             | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | model     | RobertaForSequenceClassification | 124 M  | eval \n",
      "1 | criterion | BCEWithLogitsLoss                | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.601   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "230       Modules in eval mode\n",
      "c:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv2\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 7/7 [00:00<00:00, 10.00it/s, v_num=14]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 7/7 [00:06<00:00,  1.05it/s, v_num=14]\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)  # Use float32 for BCEWithLogitsLoss\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch])\n",
    "    }\n",
    "\n",
    "def calculate_class_weights(labels):\n",
    "    # Calculate the number of samples for each class\n",
    "    class_counts = labels.sum(axis=0)\n",
    "    \n",
    "    # Total number of samples\n",
    "    total_samples = labels.shape[0]\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "    \n",
    "    # Replace infinite weights with 0\n",
    "    class_weights[np.isinf(class_weights)] = 0\n",
    "    \n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class ReBERTaClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes, class_weights=None):\n",
    "        super(ReBERTaClassifier, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "        if class_weights is not None:\n",
    "            self.criterion = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "        else:\n",
    "            self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "# Sample usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your DataFrame `train_df`\n",
    "    train_df = train_df.sample(100, random_state=RANDOM_SEED)\n",
    "    texts = train_df['comment_text'].tolist()\n",
    "    labels = train_df[LABEL_COLUMNS].values  # Ensure this is multi-label (2D array)\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_weights = calculate_class_weights(labels)\n",
    "\n",
    "    # Define number of classes based on labels\n",
    "    num_labels = labels.shape[1]  # Get number of classes from labels shape\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", clean_up_tokenization_spaces=True)\n",
    "    max_length = 32  # Define your max_length\n",
    "\n",
    "    # Create the dataset and dataloader\n",
    "    dataset = TextDataset(texts, labels, tokenizer, max_length)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True, \n",
    "                                  collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ReBERTaClassifier(num_classes=num_labels, class_weights=class_weights)\n",
    "\n",
    "    # Train the model\n",
    "    trainer = pl.Trainer(max_epochs=5)\n",
    "    trainer.fit(model, train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1070 Ti'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[LABEL_COLUMNS].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN_COUNT = 128\n",
    "MODEL_NAME = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReBERTaClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ReBERTaClassifier, self).__init__()\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = ReBERTaClassifier(num_classes=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentsDataset(Dataset):\n",
    "\n",
    "  def __init__(\n",
    "    self, \n",
    "    data: pd.DataFrame, \n",
    "    tokenizer: RobertaTokenizer, \n",
    "    max_token_len: int = MAX_TOKEN_COUNT\n",
    "  ):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data = data\n",
    "    self.max_token_len = max_token_len\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index: int):\n",
    "    data_row = self.data.iloc[index]\n",
    "\n",
    "    comment_text = data_row.comment_text\n",
    "    labels = data_row[LABEL_COLUMNS]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      comment_text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_token_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "      comment_text=comment_text,\n",
    "      input_ids=encoding[\"input_ids\"].flatten(),\n",
    "      attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "      labels=torch.FloatTensor(labels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ToxicCommentsDataset(\n",
    "  train_df,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights calculation\n",
    "class_counts = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum().to_list()\n",
    "total_count = df.shape[0]\n",
    "class_weights = [total_count / count if count > 0 else 0 for count in class_counts]\n",
    "class_weights = np.array(class_weights) / np.sum(class_weights)\n",
    "class_weights_tensor = torch.FloatTensor(class_weights)\n",
    "\n",
    "class_weights_tensor = torch.FloatTensor(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_df, test_df, tokenizer, batch_size=8, max_token_len=128, num_workers=2):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        #self.num_workers = num_workers  # Set num_workers for parallel data loading\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            # Set up your training dataset\n",
    "            self.train_dataset = ToxicCommentsDataset(\n",
    "                self.train_df,\n",
    "                self.tokenizer,\n",
    "                self.max_token_len\n",
    "            )\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            # Set up your test dataset\n",
    "            self.test_dataset = ToxicCommentsDataset(\n",
    "                self.test_df,\n",
    "                self.tokenizer,\n",
    "                self.max_token_len\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            #num_workers=self.num_workers,  # Add num_workers for parallelism\n",
    "            pin_memory=True  # Optimize GPU performance if using GPU\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            #num_workers=self.num_workers,  # Add num_workers for parallelism\n",
    "            pin_memory=True,\n",
    "            #persistent_workers=True  # Keep workers persistent between epochs\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            #num_workers=self.num_workers,  # Add num_workers for parallelism\n",
    "            pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "data_module = ToxicCommentDataModule(\n",
    "  train_df,\n",
    "  val_df,\n",
    "  tokenizer,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta-base'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentTagger(pl.LightningModule):\n",
    "\n",
    "  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n",
    "    super().__init__()\n",
    "    self.model = RobertaModel.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "    self.classifier = nn.Linear(self.model.config.hidden_size, n_classes)\n",
    "    self.n_training_steps = n_training_steps\n",
    "    self.n_warmup_steps = n_warmup_steps\n",
    "    self.criterion = nn.BCELoss()\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels=None):\n",
    "    output = self.model(input_ids, attention_mask=attention_mask)\n",
    "    output = self.classifier(output.pooler_output)\n",
    "    output = torch.sigmoid(output)    \n",
    "    loss = 0\n",
    "    if labels is not None:\n",
    "        loss = self.criterion(output, labels)\n",
    "    return loss, output\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "    return loss\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "    return loss\n",
    "\n",
    "  def on_train_epoch_end(self, outputs):\n",
    "    \n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for output in outputs:\n",
    "      for out_labels in output[\"labels\"].detach().cpu():\n",
    "        labels.append(out_labels)\n",
    "      for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "        predictions.append(out_predictions)\n",
    "\n",
    "    labels = torch.stack(labels).int()\n",
    "    predictions = torch.stack(predictions)\n",
    "\n",
    "    for i, name in enumerate(LABEL_COLUMNS):\n",
    "      class_roc_auc = AUROC(predictions[:, i], labels[:, i])\n",
    "      self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "\n",
    "    optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "      optimizer,\n",
    "      num_warmup_steps=self.n_warmup_steps,\n",
    "      num_training_steps=self.n_training_steps\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "      optimizer=optimizer,\n",
    "      lr_scheduler=dict(\n",
    "        scheduler=scheduler,\n",
    "        interval='step'\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch=len(train_df) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9332, 46660)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ToxicCommentTagger(\n",
    "  n_classes=len(LABEL_COLUMNS),\n",
    "  n_warmup_steps=warmup_steps,\n",
    "  n_training_steps=total_training_steps \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "  dirpath=\"checkpoints\",\n",
    "  filename=\"best-checkpoint\",\n",
    "  save_top_k=1,\n",
    "  verbose=True,\n",
    "  monitor=\"val_loss\",\n",
    "  mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"lightning_logs\\\\RoBERTa\", name=\"toxic-comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    max_epochs=N_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type         | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | model      | RobertaModel | 124 M  | eval \n",
      "1 | classifier | Linear       | 4.6 K  | train\n",
      "2 | criterion  | BCELoss      | 0      | train\n",
      "----------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.601   Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc481219f47148a784d33814b117b72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1025\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1052\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1049\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1052\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    395\u001b[0m )\n\u001b[1;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Gintares_Projektai\\Toxic-Comment-Classification\\venv\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[116], line 29\u001b[0m, in \u001b[0;36mToxicCommentTagger.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m---> 29\u001b[0m   input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     30\u001b[0m   attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     31\u001b[0m   labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, validation_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
